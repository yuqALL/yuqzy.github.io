<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[图像匹配问题]]></title>
    <url>%2F3D_Vision_4%2F</url>
    <content type="text"><![CDATA[图像匹配问题 Chapter 4 Image primitives and correspondence 本部分主要讲述图像匹配问题,研究如何从图像反过来重建3D场景。这是一个非常复杂的过程，往往是无法从一张图像复原3D场景的，在后续的内容将会介绍到的双目视觉及多目视觉可以在一定程度解决问题。 一张图片往往由环境的 几何信息（geometry of the scene） 和 光线分布（light distribution） 共同组成，这两者在单一图片中往往是难以分割的，我们不能将他们分开来单独讨论。一个物体可以由特定的光线分布构成一幅图像，而另一个物体通过另一种光线分布可能形成与前面相同的图像（个人理解，这里可以想象一下镜子），很简单的例子就是形成的图像本身。物体形成的图像与人眼看到的真实世界几乎一致，但却不是真实的世界。 但是对同一场景，如果我们可以从不同的角度、位置拍摄图像，那么我们就有可能从多张图片中获取场景的几何信息。实际上，虽然每幅图像由相机的位置决定，但是场景的几何信息和光线分布是不变的（假设拍摄期间场景没有变化）。当我们从不同的角度观察时，很容易观察到场景图像的变化：远处的物体移动慢，近处物体移动快。但是当我们将相机对着平面图像运动时，我们会发现所有点朝着相同的方向移动。这是因为3D场景比平面图像多了一个维度信息，一般称作深度信息。 从上述过程中，我们可以直观的感受到，要想从图片中重建三维世界，我们必须对同一场景拍摄多张图片。但是，对于同一场景不同图片上的点，如何建立它们之间的对应关系？这是重建场景的核心问题，被称为匹配问题（correspondence problem）。 对于大部分场景而言，上述问题往往是无解的。如果图像之间的光线分布和几何信息可以随意改变，那么将没有办法建立不同图像上点之间的对应关系。例如，我们让一个白色的大理石球体旋转，每张图片几乎都一样，那怎么建立匹配呢？那如果这里改成静止的镜面反射的球体，让光源移动，尽管物体上的点是静止的，但是不同的图片上看见的外观一点都不一样。甚至如果光线和几何信息都保持不变，而物体的表面具有各向异性的反射特性，以至于从不同的视角看到不一样形貌，建立匹配将会非常的困难。 本节内容就是聚焦在什么样的情况下，匹配问题能够解决，并且是十分容易解决的。 4.1 Correspondence between images 在一幅图像中重点标出一个点（3D point），并在另一幅图像中找到这个点，这就是建立匹配模型要达到的目标。 假设有一幅图像标记为$I_1$，在电脑中常常表现为一个二维矩阵，将这个二维区域记作$\Omega$，其内的值记作$I(x)$(强度 irradiance)(x表示像素位置，一般是一个矢量)，$I(x)$一般是一个正数。故可以如下的关系：$$ I_1 : \Omega \subset R^2 \rightarrow R_+ $$ $$ x \mapsto I(x)$$]]></content>
      <categories>
        <category>language</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[htmlcleaner代码学习]]></title>
    <url>%2Fhtmlcleaner%2F</url>
    <content type="text"><![CDATA[htmlcleaner代码学习相比Jsoup，htmlcleaner支持XPath进行抽取，也是挺有用的。 htmlcleaner托管在sourceforge下http://htmlcleaner.sourceforge.net/‎，由于某种原因，访问sourceforge不是那么顺畅，最后选了这个比较新的github上的fork:https://github.com/amplafi/htmlcleaner。 htmlcleaner的包结构与Jsoup还是有些差距，一开始就被一字排开的类给吓到了。 htmlcleaner仍然有一套自己的树结构，继承自:HtmlNode。但是它提供了到org.w3c.dom.Document和org.jdom2.Document的转换。 HtmlTokenizer是词法分析部分，有状态但是没用状态机，而是用了一些基本类型来保存状态，例如： public class HtmlTokenizer { private BufferedReader _reader; private char[] _working = new char[WORKING_BUFFER_SIZE]; private transient int _pos; private transient int _len = -1; private transient int _row = 1; private transient int _col = 1; private transient StringBuffer _saved = new StringBuffer(512); private transient boolean _isLateForDoctype; private transient DoctypeToken _docType; private transient TagToken _currentTagToken; private transient List&lt;BaseToken&gt; _tokenList = new ArrayList&lt;BaseToken&gt;(); private transient Set&lt;String&gt; _namespacePrefixes = new HashSet&lt;String&gt;(); private boolean _asExpected = true; private boolean _isScriptContext; } 浓烈的面向过程编程的味道。 Tokenize之后就是简单的用栈将树组合起来。 测试了一下，一个44k的文档，用Jsoup做parse是3.5ms，而htmlcleaner是7.9ms，差距在一倍左右。 XPath部分也是云里雾里，]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之二-DOM相关对象]]></title>
    <url>%2Fjsoup%20learning%202%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之二-DOM相关对象之前在文章中说到，Jsoup使用了一套自己的DOM对象体系，和Java XML API互不兼容。这样做的好处是从XML的API里解脱出来，使得代码精炼了很多。这篇文章会说明Jsoup的DOM结构，DOM的遍历方式。在下一篇文章，我会并结合这两个基础，分析一下Jsoup的HTML输出功能。 DOM结构相关类我们先来看看nodes包的类图： 这里可以看到，核心无疑是Node类。 Node类是一个抽象类，它代表DOM树中的一个节点，它包含： 父节点parentNode以及子节点childNodes的引用 属性值集合attributes 页面的uribaseUri，用于修正相对地址为绝对地址 在兄弟节点中的位置siblingIndex，用于进行DOM操作 Node里面包含一些获取属性、父子节点、修改元素的方法，其中比较有意思的是absUrl()。我们知道，在很多html页面里，链接会使用相对地址，我们有时会需要将其转变为绝对地址。Jsoup的解决方案是在attr()的参数开始加”abs:”，例如attr(“abs:href”)，而absUrl()就是其实现方式。我写的爬虫框架webmagic里也用到了类似功能，当时是自己手写的，看到Jsoup的实现，才发现自己是白费劲了，代码如下： 123456789101112131415161718URL base;try &#123; try &#123; base = new URL(baseUri); &#125; catch (MalformedURLException e) &#123; // the base is unsuitable, but the attribute may be abs on its own, so try that URL abs = new URL(relUrl); return abs.toExternalForm(); &#125; // workaround: java resolves '//path/file + ?foo' to '//path/?foo', not '//path/file?foo' as desired if (relUrl.startsWith("?")) relUrl = base.getPath() + relUrl; // java URL自带的相对路径解析 URL abs = new URL(base, relUrl); return abs.toExternalForm();&#125; catch (MalformedURLException e) &#123; return "";&#125; Node还有一个比较值得一提的方法是abstract String nodeName()，这个相当于定义了节点的类型名(例如Document是’#Document’，Element则是对应的TagName)。 Element也是一个重要的类，它代表的是一个HTML元素。它包含一个字段tag和classNames。classNames是”class”属性解析出来的集合，因为CSS规范里，”class”属性允许设置多个，并用空格隔开，而在用Selector选择的时候，即使只指定其中一个，也能够选中其中的元素。所以这里就把”class”属性展开了。Element还有选取元素的入口，例如select、getElementByXXX，这些都用到了select包中的内容，这个留到下篇文章select再说。 Document是代表整个文档，它也是一个特殊的Element，即根节点。Document除了Element的内容，还包括一些输出的方法。 Document还有一个属性quirksMode，大致意思是定义处理非标准HTML的几个级别，这个留到以后分析parser的时候再说。 DOM树的遍历Node还有一些方法，例如outerHtml()，用作节点及文档HTML的输出，用到了树的遍历。在DOM树的遍历上，用到了NodeVisitor和NodeTraversor来对树的进行遍历。NodeVisitor在上一篇文章提到过了，head()和tail()分别是遍历开始和结束时的方法，而NodeTraversor的核心代码如下： 1234567891011121314151617181920212223242526public void traverse(Node root) &#123; Node node = root; int depth = 0; //这里对树进行后序(深度优先)遍历 while (node != null) &#123; //开始遍历node visitor.head(node, depth); if (node.childNodeSize() &gt; 0) &#123; node = node.childNode(0); depth++; &#125; else &#123; //没有下一个兄弟节点，退栈 while (node.nextSibling() == null &amp;&amp; depth &gt; 0) &#123; visitor.tail(node, depth); node = node.parent(); depth--; &#125; //结束遍历 visitor.tail(node, depth); if (node == root) break; node = node.nextSibling(); &#125; &#125;&#125; 这里使用循环+回溯来替换掉了我们常用的递归方式，从而避免了栈溢出的风险。 实际上，Jsoup的Selector机制也是基于NodeVisitor来实现的，可以说NodeVisitor是更加底层和灵活的API。 在下一篇博客我会讲讲Document的输出。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之一-概述]]></title>
    <url>%2Fjsoup%20learning%201%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之一-概述 今天看到一个用python写的抽取正文的东东，美滋滋的用Java实现了一番，放到了webmagic里，然后发现Jsoup里已经有了…觉得自己各种不靠谱啊！算了，静下心来学学好东西吧！ Jsoup是Java世界用作html解析和过滤的不二之选。支持将html解析为DOM树、支持CSS Selector形式选择、支持html过滤，本身还附带了一个Http下载器。 概述Jsoup的代码相当简洁，Jsoup总共53个类，且没有任何第三方包的依赖，对比最终发行包9.8M的SAXON，实在算得上是短小精悍了。 1234567jsoup├── examples #样例，包括一个将html转为纯文本和一个抽取所有链接地址的例子。 ├── helper #一些工具类，包括读取数据、处理连接以及字符串转换的工具├── nodes #DOM节点定义├── parser #解析html并转换为DOM树├── safety #安全相关，包括白名单及html过滤└── select #选择器，支持CSS Selector以及NodeVisitor格式的遍历 使用Jsoup的入口是Jsoup类。examples包里提供了两个例子，解析html后，分别用CSS Selector以及NodeVisitor来操作Dom元素。 这里用ListLinks里的例子来说明如何调用Jsoup： 12345678910111213141516public static void main(String[] args) throws IOException &#123; Validate.isTrue(args.length == 1, "usage: supply url to fetch"); String url = args[0]; print("Fetching %s...", url); // 下载url并解析成html DOM结构 Document doc = Jsoup.connect(url).get(); // 使用select方法选择元素，参数是CSS Selector表达式 Elements links = doc.select("a[href]"); print("\nLinks: (%d)", links.size()); for (Element link : links) &#123; //使用abs:前缀取绝对url地址 print(" * a: &lt;%s&gt; (%s)", link.attr("abs:href"), trim(link.text(), 35)); &#125;&#125; Jsoup使用了自己的一套DOM代码体系，这里的Elements、Element等虽然名字和概念都与Java XML APIorg.w3c.dom类似，但并没有代码层面的关系。就是说你想用XML的一套API来操作Jsoup的结果是办不到的，但是正因为如此，才使得Jsoup可以抛弃xml里一些繁琐的API，使得代码更加简单。 还有一种方式是通过NodeVisitor来遍历DOM树，这个在对整个html做分析和替换时比较有用： 12345678public interface NodeVisitor &#123; //遍历到节点开始时，调用此方法 public void head(Node node, int depth); //遍历到节点结束时(所有子节点都已遍历完)，调用此方法 public void tail(Node node, int depth);&#125; HtmlToPlainText的例子说明了如何使用NodeVisitor来遍历DOM树，将html转化为纯文本，并将需要换行的标签替换为换行\n： 123456789101112131415161718192021public static void main(String... args) throws IOException &#123; Validate.isTrue(args.length == 1, "usage: supply url to fetch"); String url = args[0]; // fetch the specified URL and parse to a HTML DOM Document doc = Jsoup.connect(url).get(); HtmlToPlainText formatter = new HtmlToPlainText(); String plainText = formatter.getPlainText(doc); System.out.println(plainText);&#125;public String getPlainText(Element element) &#123; //自定义一个NodeVisitor - FormattingVisitor FormattingVisitor formatter = new FormattingVisitor(); //使用NodeTraversor来装载FormattingVisitor NodeTraversor traversor = new NodeTraversor(formatter); //进行遍历 traversor.traverse(element); return formatter.toString();&#125; 下一节将从DOM结构开始对Jsoup代码进行分析。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之四-parser(上)]]></title>
    <url>%2Fjsoup%20learning%204%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之四-parser(上)作为Java世界最好的HTML 解析库，Jsoup的parser实现非常具有代表性。这部分也是Jsoup最复杂的部分，需要一些数据结构、状态机乃至编译器的知识。好在HTML语法不复杂，解析只是到DOM树为止，所以作为编译器入门倒是挺合适的。这一块不要指望囫囵吞枣，我们还是泡一杯咖啡，细细品味其中的奥妙吧。 基础知识编译器将计算机语言转化为另一种计算机语言(通常是更底层的语言，例如机器码、汇编、或者JVM字节码)的过程就叫做编译(compile)。编译器(Compiler)是计算机科学的一个重要领域，已经有很多年历史了，而最近各种通用语言层出不穷，加上跨语言编译的兴起、DSL概念的流行，都让编译器变成了一个很时髦的东西。 编译器领域相关有三本公认的经典书籍，龙书《Compilers: Principles, Techniques, and Tools 》，虎书《Modern Compiler Implementation in X (X表示各种语言)》，鲸书《Advanced Compiler Design and Implementation》。其中龙书是编译理论方面公认的不二之选，而后面两本则对实践更有指导意义。另外@装配脑袋有个很好的编译器入门系列博客：http://www.cnblogs.com/Ninputer/archive/2011/06/07/2074632.html 编译器的基本流程如下： 其中词法分析、语法分析、语义分析这部分又叫编译器的前端(front-end)，而此后的中间代码生成直到目标生成、优化等属于编译器的后端(back-end)。编译器的前端技术已经很成熟了，也有yacc这样的工具来自动进行词法、语法分析(Java里也有一个类似的工具ANTLR)，而后端技术更加复杂，也是目前编译器研究的重点。 说了这么多，回到咱们的HTML上来。HTML是一种声明式的语言，可以理解它的最终的输出是浏览器里图形化的页面，而并非可执行的目标语言，因此我将这里的Translate改为了Render。 在Jsoup(包括类似的HTML parser)里，只做了Lex(词法分析)、Parse(语法分析)两步，而HTML parse最终产出结果，就是DOM树。至于HTML的语义解析以及渲染，不妨看看携程UED团队的这篇文章：《浏览器是怎样工作的：渲染引擎，HTML解析》。 状态机Jsoup的词法分析和语法分析都用到了状态机。状态机可以理解为一个特殊的程序模型，例如经常跟我们打交道的正则表达式就是用状态机实现的。 它由状态(state)和转移(transition)两部分构成。根据状态转移的可能性，状态机又分为DFA(确定有限状态机)和NFA(非确定有限状态自动机)。这里拿一个最简单的正则表达式”a[b]*”作为例子，我们先把它映射到一个状态机DFA，大概是这样子： 状态机本身是一个编程模型，这里我们尝试用程序去实现它，那么最直接的方式大概是这样： 123456789101112131415161718192021public void process(StringReader reader) throws StringReader.EOFException &#123; char ch; switch (state) &#123; case Init: ch = reader.read(); if (ch == 'a') &#123; state = State.AfterA; accum.append(ch); &#125; break; case AfterA: ... break; case AfterB: ... break; case Accept: ... break; &#125;&#125; 这样写简单的状态机倒没有问题，但是复杂情况下就有点难受了。还有一种标准的状态机解法，先建立状态转移表，然后使用这个表建立状态机。这个方法的问题就是，只能做纯状态转移，无法在代码级别操作输入输出。 Jsoup里则使用了状态模式来实现状态机，初次看到时，确实让人眼前一亮。状态模式是设计模式的一种，它将状态和对应的行为绑定在一起。而在状态机的实现过程中，使用它来实现状态转移时的处理再合适不过了。 “a[b]*”的例子的状态模式实现如下，这里采用了与Jsoup相同的方式，用到了枚举来实现状态模式： 1234567891011121314151617181920212223242526272829303132333435public class StateModelABStateMachine implements ABStateMachine &#123; State state; StringBuilder accum; enum State &#123; Init &#123; @Override public void process(StateModelABStateMachine stateModelABStateMachine, StringReader reader) throws StringReader.EOFException &#123; char ch = reader.read(); if (ch == 'a') &#123; stateModelABStateMachine.state = AfterA; stateModelABStateMachine.accum.append(ch); &#125; &#125; &#125;, Accept &#123; ... &#125;, AfterA &#123; ... &#125;, AfterB &#123; ... &#125;; public void process(StateModelABStateMachine stateModelABStateMachine, StringReader reader) throws StringReader.EOFException &#123; &#125; &#125; public void process(StringReader reader) throws StringReader.EOFException &#123; state.process(this, reader); &#125;&#125; 本文中提到的几种状态机的完整实现在这个仓库的https://github.com/code4craft/jsoup-learning/tree/master/src/main/java/us/codecraft/learning/automata路径下。 下一篇文章将从Jsoup的词法分析器开始来讲状态机的使用。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之三-Document的输出]]></title>
    <url>%2Fjsoup%20learning%203%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之三-Document的输出Jsoup官方说明里，一个重要的功能就是output tidy HTML。这里我们看看Jsoup是如何输出HTML的。 HTML相关知识分析代码前，我们不妨先想想，”tidy HTML”到底包括哪些东西： 换行，块级标签习惯上都会独占一行 缩进，根据HTML标签嵌套层数，行首缩进会不同 严格的标签闭合，如果是可以自闭合的标签并且没有内容，则进行自闭合 HTML实体的转义 这里要补充一下HTML标签的知识。HTML Tag可以分为block和inline两类。关于Tag的inline和block的定义可以参考http://www.w3schools.com/html/html_blocks.asp，而Jsoup的Tag类则是对Java开发者非常好的学习资料。 12345678910111213141516171819202122232425262728293031// internal static initialisers:// prepped from http://www.w3.org/TR/REC-html40/sgml/dtd.html and other sources//block tags，需要换行private static final String[] blockTags = &#123; "html", "head", "body", "frameset", "script", "noscript", "style", "meta", "link", "title", "frame", "noframes", "section", "nav", "aside", "hgroup", "header", "footer", "p", "h1", "h2", "h3", "h4", "h5", "h6", "ul", "ol", "pre", "div", "blockquote", "hr", "address", "figure", "figcaption", "form", "fieldset", "ins", "del", "s", "dl", "dt", "dd", "li", "table", "caption", "thead", "tfoot", "tbody", "colgroup", "col", "tr", "th", "td", "video", "audio", "canvas", "details", "menu", "plaintext"&#125;;//inline tags，无需换行private static final String[] inlineTags = &#123; "object", "base", "font", "tt", "i", "b", "u", "big", "small", "em", "strong", "dfn", "code", "samp", "kbd", "var", "cite", "abbr", "time", "acronym", "mark", "ruby", "rt", "rp", "a", "img", "br", "wbr", "map", "q", "sub", "sup", "bdo", "iframe", "embed", "span", "input", "select", "textarea", "label", "button", "optgroup", "option", "legend", "datalist", "keygen", "output", "progress", "meter", "area", "param", "source", "track", "summary", "command", "device"&#125;;//emptyTags是不能有内容的标签，这类标签都是可以自闭合的private static final String[] emptyTags = &#123; "meta", "link", "base", "frame", "img", "br", "wbr", "embed", "hr", "input", "keygen", "col", "command", "device"&#125;;private static final String[] formatAsInlineTags = &#123; "title", "a", "p", "h1", "h2", "h3", "h4", "h5", "h6", "pre", "address", "li", "th", "td", "script", "style", "ins", "del", "s"&#125;;//在这些标签里，需要保留空格private static final String[] preserveWhitespaceTags = &#123; "pre", "plaintext", "title", "textarea"&#125;; 另外，Jsoup的Entities类里包含了一些HTML实体转义的东西。这些转义的对应数据保存在entities-full.properties和entities-base.properties里。 Jsoup的格式化实现在Jsoup里，直接调用Document.toString()(继承自Element)，即可对文档进行输出。另外OutputSettings可以控制输出格式，主要是prettyPrint(是否重新格式化)、outline(是否强制所有标签换行)、indentAmount(缩进长度)等。 里面的继承和互相调用关系略微复杂，大概是这样子： Document.toString()=&gt;Document.outerHtml()=&gt;Element.html()，最终Element.html()又会循环调用所有子元素的outerHtml()，拼接起来作为输出。 1234private void html(StringBuilder accum) &#123; for (Node node : childNodes) node.outerHtml(accum);&#125; 而outerHtml()会使用一个OuterHtmlVisitor对所以子节点做遍历，并拼装起来作为结果。 123protected void outerHtml(StringBuilder accum) &#123; new NodeTraversor(new OuterHtmlVisitor(accum, getOutputSettings())).traverse(this); &#125; OuterHtmlVisitor会对所有子节点做遍历，并调用node.outerHtmlHead()和node.outerHtmlTail两个方法。 12345678910111213private static class OuterHtmlVisitor implements NodeVisitor &#123; private StringBuilder accum; private Document.OutputSettings out; public void head(Node node, int depth) &#123; node.outerHtmlHead(accum, depth, out); &#125; public void tail(Node node, int depth) &#123; if (!node.nodeName().equals("#text")) // saves a void hit. node.outerHtmlTail(accum, depth, out); &#125;&#125; 我们终于找到了真正工作的代码，node.outerHtmlHead()和node.outerHtmlTail。Jsoup里每种Node的输出方式都不太一样，这里只讲讲两种主要节点：Element和TextNode。Element是格式化的主要对象，它的两个方法代码如下： 1234567891011121314151617181920212223242526void outerHtmlHead(StringBuilder accum, int depth, Document.OutputSettings out) &#123; if (accum.length() &gt; 0 &amp;&amp; out.prettyPrint() &amp;&amp; (tag.formatAsBlock() || (parent() != null &amp;&amp; parent().tag().formatAsBlock()) || out.outline()) ) //换行并调整缩进 indent(accum, depth, out); accum .append("&lt;") .append(tagName()); attributes.html(accum, out); if (childNodes.isEmpty() &amp;&amp; tag.isSelfClosing()) accum.append(" /&gt;"); else accum.append("&gt;");&#125;void outerHtmlTail(StringBuilder accum, int depth, Document.OutputSettings out) &#123; if (!(childNodes.isEmpty() &amp;&amp; tag.isSelfClosing())) &#123; if (out.prettyPrint() &amp;&amp; (!childNodes.isEmpty() &amp;&amp; ( tag.formatAsBlock() || (out.outline() &amp;&amp; (childNodes.size()&gt;1 || (childNodes.size()==1 &amp;&amp; !(childNodes.get(0) instanceof TextNode)))) ))) //换行并调整缩进 indent(accum, depth, out); accum.append("&lt;/").append(tagName()).append("&gt;"); &#125;&#125; 而ident方法的代码只有一行： 1234protected void indent(StringBuilder accum, int depth, Document.OutputSettings out) &#123; //out.indentAmount()是缩进长度，默认是1 accum.append("\n").append(StringUtil.padding(depth * out.indentAmount()));&#125; 代码简单明了，就没什么好说的了。值得一提的是，StringUtil.padding()方法为了减少字符串生成，把常用的缩进保存到了一个数组中。 好了，水了一篇文章，下一篇将比较有技术含量的parser部分。 另外，通过本节的学习，我们学到了要把StringBuilder命名为accum，而不是sb。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之五-parser(中)]]></title>
    <url>%2Fjsoup%20learning%205%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之五-parser(中)上一篇文章讲到了状态机和词法分析的基本知识，这一节我们来分析Jsoup是如何进行词法分析的。 代码结构先介绍以下parser包里的主要类： Parser Jsoup parser的入口facade，封装了常用的parse静态方法。可以设置maxErrors，用于收集错误记录，默认是0，即不收集。与之相关的类有ParseError,ParseErrorList。基于这个功能，我写了一个PageErrorChecker来对页面做语法检查，并输出语法错误。 Token 保存单个的词法分析结果。Token是一个抽象类，它的实现有Doctype,StartTag,EndTag,Comment,Character,EOF6种，对应6种词法类型。 Tokeniser 保存词法分析过程的状态及结果。比较重要的两个字段是state和emitPending，前者保存状态，后者保存输出。其次还有tagPending/doctypePending/commentPending，保存还没有填充完整的Token。 CharacterReader 对读取字符的逻辑的封装，用于Tokenize时候的字符输入。CharacterReader包含了类似NIO里ByteBuffer的consume()、unconsume()、mark()、rewindToMark()，还有高级的consumeTo()这样的用法。 TokeniserState 用枚举实现的词法分析状态机。 HtmlTreeBuilder 语法分析，通过token构建DOM树的类。 HtmlTreeBuilderState 语法分析状态机。 TokenQueue 虽然披了个Token的马甲，其实是在query的时候用到，留到select部分再讲。 词法分析状态机现在我们来讲讲HTML的词法分析过程。这里借用一下http://ued.ctrip.com/blog/?p=3295里的图，图中描述了一个Tag标签的状态转移过程， 这里忽略了HTML注释、实体以及属性，只保留基本的开始/结束标签，例如下面的HTML: &lt;div&gt;test&lt;/div&gt; Jsoup里词法分析比较复杂，我从里面抽取出了对应的部分，就成了我们的miniSoupLexer(这里省略了部分代码，完整代码可以看这里MiniSoupTokeniserState)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748enum MiniSoupTokeniserState implements ITokeniserState &#123; /** * 什么层级都没有的状态 * ⬇ * &lt;div&gt;test&lt;/div&gt; * ⬇ * &lt;div&gt;test&lt;/div&gt; */ Data &#123; // in data state, gather characters until a character reference or tag is found public void read(Tokeniser t, CharacterReader r) &#123; switch (r.current()) &#123; case '&lt;': t.advanceTransition(TagOpen); break; case eof: t.emit(new Token.EOF()); break; default: String data = r.consumeToAny('&amp;', '&lt;', nullChar); t.emit(data); break; &#125; &#125; &#125;, /** * ⬇ * &lt;div&gt;test&lt;/div&gt; */ TagOpen &#123; ... &#125;, /** * ⬇ * &lt;div&gt;test&lt;/div&gt; */ EndTagOpen &#123; ... &#125;, /** * ⬇ * &lt;div&gt;test&lt;/div&gt; */ TagName &#123; ... &#125;;&#125; 参考这个程序，可以看到Jsoup的词法分析的大致思路。分析器本身的编写是比较繁琐的过程，涉及属性值(区分单双引号)、DocType、注释、HTML实体，以及一些错误情况。不过了解了其思路，代码实现也是按部就班的过程。 下一节开始介绍语法分析部分。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之七-实现一个CSS Selector]]></title>
    <url>%2Fjsoup%20learning%207%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之七-实现一个CSS Selector 当当当！终于来到了Jsoup的特色：CSS Selector部分。selector也是我写的爬虫框架webmagic开发的一个重点。附上一张street fighter的图，希望以后webmagic也能挑战Jsoup! select机制Jsoup的select包里，类结构如下： 在最开始介绍Jsoup的时候，就已经说过NodeVisitor和Selector了。Selector是select部分的对外facade，而NodeVisitor则是遍历树的底层API，CSS Selector也是根据NodeVisitor实现的遍历。 Jsoup的select核心是Evaluator。Selector所传递的表达式，会经过QueryParser，最终编译成一个Evaluator。Evaluator是一个抽象类，它只有一个方法： 1public abstract boolean matches(Element root, Element element); 注意这里传入了root，是为了某些情况下对树进行遍历时用的。 Evaluator的设计简洁明了，所有的Selector表达式单词都会编译到对应的Evaluator。例如#xx对应Id，.xx对应Class，[]对应Attribute。这里补充一下w3c的CSS Selector规范：http://www.w3.org/TR/CSS2/selector.html 当然，只靠这几个还不够，Jsoup还定义了CombiningEvaluator(对Evaluator进行And/Or组合)，StructuralEvaluator(结合DOM树结构进行筛选)。 这里我们可能最关心的是，“div ul li”这样的父子结构是如何实现的。这个的实现方式在StructuralEvaluator.Parent中，贴一下代码了： static class Parent extends StructuralEvaluator { public Parent(Evaluator evaluator) { this.evaluator = evaluator; } public boolean matches(Element root, Element element) { if (root == element) return false; Element parent = element.parent(); while (parent != root) { if (evaluator.matches(root, parent)) return true; parent = parent.parent(); } return false; } } 这里Parent包含了一个evaluator属性，会根据这个evaluator去验证所有父节点。注意Parent是可以嵌套的，所以这个表达式”div ul li”最终会编译成And(Parent(And(Parent(Tag(&quot;div&quot;))，Tag(&quot;ul&quot;)),Tag(&quot;li&quot;)))这样的Evaluator组合。 select部分比想象的要简单，代码可读性也很高。经过了parser部分的研究，这部分应该算是驾轻就熟了。 关于webmagic的后续打算webmagic是一个爬虫框架，它的Selector是用于抓取HTML中指定的文本，其机制和Jsoup的Evaluator非常像，只不过webmagic暂时是将Selector封装成较简单的API，而Evaluator直接上了表达式。之前也考虑过自己定制DSL来写一个HTML，现在看了Jsoup的源码，实现能力算是有了，但是引入DSL，实现只是一小部分，如何让DSL易写易懂才是难点。 其实看了Jsoup的源码，精细程度上比webmagic要好得多了，基本每个类都对应一个真实的概念抽象，可能以后会在这方面下点工夫。 下篇文章将讲最后一部分：白名单及HTML过滤机制。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之六-parser(下)]]></title>
    <url>%2Fjsoup%20learning%206%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之六-parser(下)最近生活上有点忙，女儿老是半夜不睡，精神状态也不是很好。工作上的事情也谈不上顺心，有很多想法但是没有几个被认可，有些事情也不是说代码写得好就行的。算了，还是端正态度，毕竟资历尚浅，我还是继续我的。 读Jsoup源码并非无聊，目的其实是为了将webmagic做的更好一点，毕竟parser也是爬虫的重要组成部分之一。读了代码后，收获也不少，对HTML的知识也更进一步了。 DOM树产生过程这里单独将TreeBuilder部分抽出来叫做语法分析过程可能稍微不妥，其实就是根据Token生成DOM树的过程，不过我还是沿用这个编译器里的称呼了。 TreeBuilder同样是一个facade对象，真正进行语法解析的是以下一段代码： 12345678910protected void runParser() &#123; while (true) &#123; Token token = tokeniser.read(); process(token); if (token.type == Token.TokenType.EOF) break; &#125;&#125; TreeBuilder有两个子类，HtmlTreeBuilder和XmlTreeBuilder。XmlTreeBuilder自然是构建XML树的类，实现颇为简单，基本上是维护一个栈，并根据不同Token插入节点即可： 1234567891011121314151617181920212223242526@Override protected boolean process(Token token) &#123; // start tag, end tag, doctype, comment, character, eof switch (token.type) &#123; case StartTag: insert(token.asStartTag()); break; case EndTag: popStackToClose(token.asEndTag()); break; case Comment: insert(token.asComment()); break; case Character: insert(token.asCharacter()); break; case Doctype: insert(token.asDoctype()); break; case EOF: // could put some normalisation here if desired break; default: Validate.fail("Unexpected token type: " + token.type); &#125; return true; &#125; insertNode的代码大致是这个样子(为了便于展示，对方法进行了一些整合)： 12345678910111213Element insert(Token.StartTag startTag) &#123; Tag tag = Tag.valueOf(startTag.name()); Element el = new Element(tag, baseUri, startTag.attributes); stack.getLast().appendChild(el); if (startTag.isSelfClosing()) &#123; tokeniser.acknowledgeSelfClosingFlag(); if (!tag.isKnownTag()) // unknown tag, remember this is self closing for output. see above. tag.setSelfClosing(); &#125; else &#123; stack.add(el); &#125; return el;&#125; HTML解析状态机相比XmlTreeBuilder，HtmlTreeBuilder则实现较为复杂，除了类似的栈结构以外，还用到了HtmlTreeBuilderState来构建了一个状态机来分析HTML。这是为什么呢？不妨看看HtmlTreeBuilderState到底用到了哪些状态吧（在代码中中用&lt;!-- State: --&gt;标明状态）： 123456789101112131415161718192021222324252627282930313233343536373839&lt;!-- State: Initial --&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;&lt;!-- State: BeforeHtml --&gt;&lt;html lang='zh-CN' xml:lang='zh-CN' xmlns='http://www.w3.org/1999/xhtml'&gt;&lt;!-- State: BeforeHead --&gt;&lt;head&gt; &lt;!-- State: InHead --&gt; &lt;script type="text/javascript"&gt; //&lt;!-- State: Text --&gt; function xx()&#123; &#125; &lt;/script&gt; &lt;noscript&gt; &lt;!-- State: InHeadNoscript --&gt; Your browser does not support JavaScript! &lt;/noscript&gt;&lt;/head&gt;&lt;!-- State: AfterHead --&gt;&lt;body&gt;&lt;!-- State: InBody --&gt;&lt;textarea&gt; &lt;!-- State: Text --&gt; xxx&lt;/textarea&gt;&lt;table&gt; &lt;!-- State: InTable --&gt; &lt;!-- State: InTableText --&gt; xxx &lt;tbody&gt; &lt;!-- State: InTableBody --&gt; &lt;/tbody&gt; &lt;tr&gt; &lt;!-- State: InRow --&gt; &lt;td&gt; &lt;!-- State: InCell --&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/html&gt; 这里可以看到，HTML标签是有嵌套要求的，例如&lt;tr&gt;,&lt;td&gt;需要组合&lt;table&gt;来使用。根据Jsoup的代码，可以发现，HtmlTreeBuilderState做了以下一些事情： 语法检查 例如tr没有嵌套在table标签内，则是一个语法错误。当InBody状态直接出现以下tag时，则出错。Jsoup里遇到这种错误，会发现这个Token的解析并记录错误，然后继续解析下面内容，并不会直接退出。 12345678 InBody &#123; boolean process(Token t, HtmlTreeBuilder tb) &#123;if (StringUtil.in(name,"caption", "col", "colgroup", "frame", "head", "tbody", "td", "tfoot", "th", "thead", "tr")) &#123;tb.error(this);return false;&#125; &#125; 标签补全 例如head标签没有闭合，就写入了一些只有body内才允许出现的标签，则自动闭合&lt;/head&gt;。HtmlTreeBuilderState有的方法anythingElse()就提供了自动补全标签，例如InHead状态的自动闭合代码如下： 123456789101112131415 private boolean anythingElse(Token t, TreeBuilder tb) &#123; tb.process(new Token.EndTag("head")); return tb.process(t); &#125;``` 还有一种标签闭合方式，例如下面的代码： ```java private void closeCell(HtmlTreeBuilder tb) &#123; if (tb.inTableScope("td")) tb.process(new Token.EndTag("td")); else tb.process(new Token.EndTag("th")); // only here if th or td in scope &#125; 实例研究缺少标签时，会发生什么事？好了，看了这么多parser的源码，不妨回到我们的日常应用上来。我们知道，在页面里多写一个两个未闭合的标签是很正常的事，那么它们会被怎么解析呢？ 就拿&lt;div&gt;标签为例： 漏写了开始标签，只写了结束标签 1234567case EndTag: if (StringUtil.in(name,"div","dl", "fieldset", "figcaption", "figure", "footer", "header", "pre", "section", "summary", "ul")) &#123; if (!tb.inScope(name)) &#123; tb.error(this); return false; &#125; &#125; 恭喜你，这个`&lt;/div&gt;`会被当做错误处理掉，于是你的页面就毫无疑问的乱掉了！当然，如果单纯多写了一个`&lt;/div&gt;`，好像也不会有什么影响哦？(记得有人跟我讲过为了防止标签未闭合，而在页面底部多写了几个`&lt;/div&gt;`的故事) 写了开始标签，漏写了结束标签 这个情况分析起来更复杂一点。如果是无法在内部嵌套内容的标签，那么在遇到不可接受的标签时，会进行闭合。而&lt;div&gt;标签可以包括大多数标签，这种情况下，其作用域会持续到HTML结束。 好了，parser系列算是分析结束了，其间学到不少HTML及状态机内容，但是离实际使用比较远。下面开始select部分，这部分可能对日常使用更有意义一点。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jsoup代码解读之八-防御XSS攻击]]></title>
    <url>%2Fjsoup%20learning%208%2F</url>
    <content type="text"><![CDATA[Jsoup代码解读之八-防御XSS攻击 防御XSS攻击的一般原理cleaner是Jsoup的重要功能之一，我们常用它来进行富文本输入中的XSS防御。 我们知道，XSS攻击的一般方式是，通过在页面输入中嵌入一段恶意脚本，对输出时的DOM结构进行修改，从而达到执行这段脚本的目的。对于纯文本输入，过滤/转义HTML特殊字符&lt;,&gt;,&quot;,&#39;是行之有效的办法，但是如果本身用户输入的就是一段HTML文本(例如博客文章)，这种方式就不太有效了。这个时候，就是Jsoup大显身手的时候了。 在前面，我们已经知道了，Jsoup里怎么将HTML变成一棵DOM树，怎么对DOM树进行遍历，怎么对DOM文档进行输出，那么其实cleaner的实现方式，也能猜出大概了。使用Jsoup进行XSS防御，大致分为三个步骤: 将HTML解析为DOM树 这一步可以过滤掉一些企图搞破坏的非闭合标签、非正常语法等。例如一些输入，会尝试用&lt;/textarea&gt;闭合当前Tag，然后写入攻击脚本。而根据前面对Jsoup的parser的分析，这种时候，这些非闭合标签会被当做错误并丢弃。 过滤高风险标签/属性/属性值 高风险标签是指&lt;script&gt;以及类似标签，对属性/属性值进行过滤是因为某些属性值里也可以写入javascript脚本，例如onclick=&#39;alert(&quot;xss!&quot;)&#39;。 重新将DOM树输出为HTML文本 DOM树的输出，在前面(Jsoup代码解读之三)已经提到过了。 Cleaner与Whitelist对于上述的两个步骤，1、3都已经分别在parser和输出中完成，现在只剩下步骤 2：过滤高风险标签等。 Jsoup给出的答案是白名单。下面是Whitelist的部分代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class Whitelist &#123; private Set&lt;TagName&gt; tagNames; // tags allowed, lower case. e.g. [p, br, span] private Map&lt;TagName, Set&lt;AttributeKey&gt;&gt; attributes; // tag -&gt; attribute[]. allowed attributes [href] for a tag. private Map&lt;TagName, Map&lt;AttributeKey, AttributeValue&gt;&gt; enforcedAttributes; // always set these attribute values private Map&lt;TagName, Map&lt;AttributeKey, Set&lt;Protocol&gt;&gt;&gt; protocols; // allowed URL protocols for attributes private boolean preserveRelativeLinks; // option to preserve relative links&#125;``` 这里定义了标签名/属性名/属性值的白名单。而`Cleaner`是过滤的执行者。不出所料，Cleaner内部定义了`CleaningVisitor`来进行标签的过滤。CleaningVisitor的过滤过程并不改变原始DOM树的值，而是将符合条件的属性，加入到`Element destination`里去。```java private final class CleaningVisitor implements NodeVisitor &#123; private int numDiscarded = 0; private final Element root; private Element destination; // current element to append nodes to private CleaningVisitor(Element root, Element destination) &#123; this.root = root; this.destination = destination; &#125; public void head(Node source, int depth) &#123; if (source instanceof Element) &#123; Element sourceEl = (Element) source; if (whitelist.isSafeTag(sourceEl.tagName())) &#123; // safe, clone and copy safe attrs ElementMeta meta = createSafeElement(sourceEl); Element destChild = meta.el; destination.appendChild(destChild); numDiscarded += meta.numAttribsDiscarded; destination = destChild; &#125; else if (source != root) &#123; // not a safe tag, so don't add. don't count root against discarded. numDiscarded++; &#125; &#125; else if (source instanceof TextNode) &#123; TextNode sourceText = (TextNode) source; TextNode destText = new TextNode(sourceText.getWholeText(), source.baseUri()); destination.appendChild(destText); &#125; else &#123; // else, we don't care about comments, xml proc instructions, etc numDiscarded++; &#125; &#125; public void tail(Node source, int depth) &#123; if (source instanceof Element &amp;&amp; whitelist.isSafeTag(source.nodeName())) &#123; destination = destination.parent(); // would have descended, so pop destination stack &#125; &#125; &#125; 结束语至此，Jsoup的全部模块都已经写完了。Jsoup源码并不多，只有14000多行，但是实现非常精巧，在读代码的过程中，除了相关知识，还验证几个很重要的思想： 最好的代码抽象，是对现实概念的映射。 这句话在看《代码大全》的时候印象很深刻。在Jsoup里，只要有相关知识，每个类的作用都能第一时间明白其作用。 不要过度抽象 在Jsoup里，只用到了两个接口，一个是NodeVisitor，一个是Connection，其他都是用抽象类或者直接用实现类代替。记得有次面试的时候被问到我们开发中每逢一个功能，都要先定义一个接口的做法是否必要？现在的答案是没有必要，过度的抽象反而会降低代码质量。 另外，Jsoup的代码内聚性都很高，每个类的功能基本都定义在类的内部，这是一个典型的充血模型。同时有大量的facade使用，而避免了Factory、Configure等类的出现，个人感觉这点是非常好的。]]></content>
      <categories>
        <category>language</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>javaCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3维运动表达]]></title>
    <url>%2F3D_Vision_1%2F</url>
    <content type="text"><![CDATA[3维运动表达 3维运动表达 Chapter 2 Representation of a three dimensional moving scene 2.1 3维欧几里得空间(Euclidean space) 2.2 刚体运动(Rigid body motion) 2.2 旋转运动及其表达(Rotational motion and its representations) 放弃了，写的太累了！！！！！！！！！！ Chapter 2 Representation of a three dimensional moving scene 本书的第二章主要讲授刚体（不发生变形）在空间中运动过程中，其当前位置与世界坐标位置（即出发前的原始坐标）之间的变换关系，刚体在空间中的运动主要可以分为位移和旋转运动，第二章将把这两个运动分开来讨论，并在其后给出统一的表达形式。 2.1 3维欧几里得空间(Euclidean space)在起初，我们要先对一些数学符号进行一些了解： three-dimensional Euclidean space(欧几里得空间) : $E^3$ Cartesian coordinate(笛卡尔坐标)：$\mathbb{R}^3$ 对于点$p \in E^3$可以用$\mathbb{R}^3$里的3个坐标来表达：$X=[X_1,X_2,X_3]^T$。这样我们就建立了$E^3$与$\mathbb{R}^3$的一一对应关系。这里的内容主要是线性代数的知识，了解空间向量的表达，向量之间的 內积（inner product）、叉积（cross product）、范数（Euclidean norm） 等计算问题。叉积有很多特殊的性质，考虑有两个向量$u$、$v$，这两个向量均属于$\mathbb{R}^3$。那么$u$与$v$这件的叉积可表示为： $$u \times v = \begin{bmatrix} u_2v_3- u_3v_2 \ u_3v_1 - u_1v_3 \ u_1v_2 - u_2v_1 \end{bmatrix} \in \mathbb{R}^3$$ 上式结果可以由$u^T=[u_1,u_2,u_3]=u_1 i +u_2 j +u_3 k$这种方式计算得来。 向量的內积记作：$$ &lt;u,v&gt;=u^T v=u_1v_1+u_2v_2+u_3v_3 \tag{2.1} $$ 我们复习一下向量叉积的性质，我们通过书本已经了解到两个向量的叉积表达的是同时垂直于两个向量的向量，且其只想符合右手定则，手指方向由运算符左边向量指向右边向量。由此可知，向量叉积结果与任一向量相互垂直，故內积为0，而且调整向量的顺序，得到的结果与调整前符号相反，即：$$ = = 0$$$$u \times v = -v \times u$$ 由$u \times v$的运算结果，考虑到：$$u \times v = \begin{bmatrix} u_2v_3- u_3v_2 \ u_3v_1 - u_1v_3 \ u_1v_2 - u_2v_1 \end{bmatrix} = \begin{bmatrix} 0 &amp; -u_3 &amp; u_2 \ u_3&amp;0 &amp; -u_1 \ -u_2&amp; u_1&amp;0 \end{bmatrix} \begin{bmatrix} v_1 \ v_2\ v_3 \end{bmatrix} =\hat{u}v\in \mathbb{R}^3 \tag{2.2}$$ 上式中$\hat{u}$表示的是一个反对称矩阵(skew-symmetric matrix),可以知道$\hat{u}^T=-\hat{u}$.我们考虑反对称矩阵$M$,则$m_{11}$=$m_{22}$=$m_{33}=0$,并且$m_{ij}=-m_{ji}$,并且$i,j$=1,2,3。假如我们另$u_1=m_{32}$,$u_2=m_{13}$,$u_3=m_{21}$，则$\hat{u}=M$。这个3维反对称矩阵只有3个自由度，即只有3个变量的值是不确定的，我们把3维反对称矩阵空间记作$so(3)$,那么我们可以得到叉积到$so(3)$的映射： $$\times : \mathbb{R} \rightarrow so(3);$$ $$u \mapsto \hat{u}$$ 由此也可以进行反向映射，这个过程称为“vee”，从反对称矩阵$\hat{u}$中解出向量$u$: $$ \vee : so(3) \rightarrow \mathbb{R}$$ $$ M=-M^T \mapsto M^\vee =[m_{32},m_{13},m_{21}]$$ 关键在于我们通过上述过程，将向量的叉积转换为两个矩阵相乘，且这个矩阵非常容易构造。反过来，我们也可以通过反对称矩阵来构造两个向量的叉积，这让向量之间的处理更加灵活方便。 2.2 刚体运动(Rigid body motion)当一个物体在相机前方运动时，此时为了描述物体的运动轨迹，原则上，需要给出物体上每个点的运动轨迹，例如给出点随时间变化的坐标函数$X(t)$。幸运的是，对于刚体而言，我们并不需要给出其上每个粒子的运动。我们指定一个点，其上带有三维坐标系，描述这个点的运动可以看作是坐标系的运动，其运动主要分为位移和旋转。很快我们就会看到，一个点就足以表达刚体的运动轨迹。 刚体的定义：当物体运动时，其上任意两点之间的距离不随时间的变化而变化。 假设$X(t)$和$Y(t)$是任意两点$p$和$q$各自关于时间的坐标，那它们之间的距离必然满足：$$ \mid\mid X(t)-Y(t) \mid\mid = constant , \forall t \in \mathbb{R} \tag{2.3}$$换句话说，如果我们定义由$p$指向$q$的向量$v$，在物体运动时，向量$v$的范数（或者说长度）保持不变：$\mid\mid v(t) \mid\mid = constant$。由此我们可以知道，刚体运动是其上每个点坐标关于时间的变换函数的集合。我们用符合$g$表示这一过程：$$ g(t): \mathbb{R}^3 \rightarrow \mathbb{R}^3$$$$ X \mapsto g(t)(X) $$如果我们不考虑物体连续的运动路径，而只关注刚体的起始位置和终止位置，我们一般将这种变换称作刚体位移(rigid body displacement),并把它记作单映射(single mapping)（不考虑时间的变化）:$$ g: \mathbb{R}^3 \rightarrow \mathbb{R}^3$$$$ X \mapsto g(X) $$$g$函数除了变换坐标位置，同时也可以变换向量。假设由两个点$p$和$q$组成的向量$v$:$v=Y-X$；经过$g$变换之后，我们可以得到一个新的向量：$$ g_*(v)=g(Y)-g(X)$$ $g$保留了任意两点之间的距离信息，数学表达为$\mid\mid g_*(v) \mid\mid =\mid\mid v \mid\mid ，其中\forall v \in \mathbb{R}$。 但是，保持点之间的距离不变并不是刚体运动的唯一要求。事实上，有些变换虽然满足距离不变，但是却不是物理上可实现的。例如，有如下映射关系：$$ f: [X_1,X_2,X_3]^T \mapsto [X_1,X_2,-X_3]^T$$其保证了距离不变，但是方向变了，其映射关系是关于$XY$平面对称。为了排除向上述那样的映射，我们要求对于刚体运动，不仅要保证距离(distance)，同时也要保证方向(orientation)。也就是说保持向量的距离（范数）之外，必须保证它们之间的叉积不变。对于刚体运动的坐标变换也称作special Euclidean transformation。单词special象征着orientation-preserving（也就是维持刚体上向量的方向）。 定义2.3 （Rigid body motion or special Euclidean transformation). 如果一个映射$g : \mathbb{R}^3 \rightarrow \mathbb{R}^3$,满足其上任意向量的范数和任意两向量之间的叉积不变，那么他就是刚体运动或者特殊欧几里得变换。A mapping $g : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ is a rigid body motion or special Euclidean transformation if it preserves the norm and the cross product of any two vectors. 1.范数:$\mid\mid g_*(v) \mid\mid =\mid\mid v \mid\mid ，其中\forall v \in \mathbb{R}$. 2.叉积:$g_(u) \times g_(v) =g_(u \times v) ，\forall u,v \in \mathbb{R}^3$在上面刚体运动的定义中，严格要求了点之间的距离不变，那向量之间的角度保证不变了吗？虽然这没有在定义中严格的陈述，从向量的内积（点积）$&lt;.,.&gt;$可以看出向量之间的角度确实得到了保证，因为内积可以通过 极化特性(polarization identity) 以范数的形式表达：$$&lt;u,v&gt;=u^T v=u_1v_1+u_2v_2+u_3v_3 \= \frac{1}{4}( \mid\mid u+v \mid\mid^2 - \mid\mid u-v \mid\mid^2 ) \=( u_1+v_1 )^2+( u_2+v_2 )^2+( u_3+v_3 )^2-( u_1-v_1 )^2-( u_2-v_2 )^2-( u_3-v_3 )^2 \tag{2.4}$$由此，对于任何刚体运动$g$，有：$$ u^T v=g_(u) g_*(v) ,\forall u,v \in \mathbb{R}^3 \tag{2.5}$$ 换句话说，刚体运动也可以定义为既可以保证任意两向量的内积不变，同时也可以保证叉积不变。 那么刚体运动的这些性质有什么用呢？实际上刚体运动的距离以及方向维持特性意味着其上各点之间不能相对对方运动，但是点与点之间在空间可以旋转（这里不太好描述，想象一个运动的固体块，显然，其上任意两点可以相对于对方旋转，但是不可能相对于对方存在距离变化），但是这个旋转必须是正确的，不能改变任意点之间的距离。 因此，刚体运动可以描述为任意一个点的运动，以及附着在这个点上的坐标系的旋转。 基于以上讨论，我们可以在刚体的某点设立一个笛卡尔坐标系，并在刚体运动的起始位置设定一个固定的坐标系（也就是常说的世界坐标系），记录起始时刚体上某点在世界坐标系中的坐标，以及点上面附着的笛卡尔坐标系与世界坐标系之间的关系，在刚体运动过程中，保持追踪运动坐标系与世界坐标系之间的相对关系。 为了方便描述，考虑世界坐标系由3个正交向量$e_1,e_2,e_3 \in \mathbb{R}^3$组成。正交向量满足：$$e_i^Te_j=\delta_{ij}\begin{cases} \delta_{ij}=1,\ for\ i=j \\delta_{ij}=0 ,\ for \ i \neq j\end{cases} \tag{2.6}$$通常，这些向量组成右手系：$e_1 \times e_2 = e_3$。然后，经过刚体运动$g$，我们得到：$$ g_(e_i) g_(e_j)=\delta_{ij} ,\ \ g_(e_1)\times g_(e_2)=g_*(e_3) \tag{2.7}$$通过上面的运算我们可以发现，计算过后的向量仍然构成右手正交系，所以对于刚体运动，可以看作一个右手正交坐标系相对于世界坐标系的运动，这里我们把这个坐标系称作 对象系（object frame）。在图2.1中我们将会看到一个物体（图中为相机）相对于世界坐标系$W$的运动。将对象坐标系附着在起点$o$上，这里的相机的坐标系用字母$C$表示，其随着相机的运动而运动。那么此时相机在空间中的位置可以由以下两部分决定： (1) 世界坐标系原点$o$与相机坐标系原点$g(o)$之间形成的向量，我们称之为移位（translational）部分并标记为$T$。 (2) 相机坐标系$C$的坐标轴$g_(e_1),g_(e_2),g_*(e_3)$相对于世界坐标系$W$的坐标轴$e_1,e_2,e_3$的方向，我们称之为旋转（rotational）部分并标记为$R$。 实际上，在视觉上没有明显的起点$o$和参考坐标系$e_1,e_2,e_3$，你可以想象相机不动，而场景在空间运动，也可以指定场景不动，相机在空间运动，这些都不重要，只要能够表达出两者之间的相对位置关系，方便理解即可。 Remark 2.1. The set of rigid body motions, or special Euclidean transformations, is a (Lie) group, the so-called special Euclidean group, typicallydenoted as $SE(3)$. Algebraically, a group is a set $G$, with an operation of (binary) multiplication $\circ$ on elements of G which is: closed: If $g1, g2 \in G$ then also $g1 \circ g2 \in G$; associative: $(g1 \circ g2) \circ g3 = g1 \circ (g2 \circ g3)$, for all $g1, g2, g3 \in G$; unit element $e: e \circ g = g \circ e = g$, for all $g \in G$; invertible: For every element $g \in G$, there exists an element $g−1 ∈ G$ such that $g \circ g^{−1} = g^{−1} \circ g = e$. 在后面的内容中，我们将会研究如何表达特殊欧几里得集合(special Euclidean group)$SE(3)$。再具体一点来说，就是实现将$SE(3)$中的元素作为一个$n \times n$非奇异矩阵的元素，那么乘法运算可以简化为矩阵的乘法运算，这样的矩阵集合一般叫做一般线性集(general linear group)并且记作$GL(n)$。$$ \mathcal{R} : SE(3) \rightarrow GL(n) $$$$ g \mapsto \mathcal{R}(g)$$ 反过来，也可以得到：$$ \mathcal{R}(g^{-1})=\mathcal{R}(g)^{-1} ,\ \mathcal{R}(g \circ h)=\mathcal{R}(g)\mathcal{R}(h),\ \forall g,h \in SE(3). \tag{2.8}$$ 2.2 旋转运动及其表达(Rotational motion and its representations)如果我们只考虑旋转，那么相对于世界坐标系$W$，对象坐标系$C$的原点$o$与世界坐标系重合，只是坐标轴发生了偏移，如图2.2所示。]]></content>
      <categories>
        <category>language</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
</search>
